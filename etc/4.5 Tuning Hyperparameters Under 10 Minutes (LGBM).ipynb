{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1177759",
   "metadata": {},
   "source": [
    "## 9. Tuning Hyperparameters Under 10 Minutes (LGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e647e8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <b>Author : Kwang Myung Yu</b></div>\n",
    "<div style=\"text-align: right\\\"> Initial upload: 2021.10.18 </div>\n",
    "<div style=\"text-align: right\\\"> Last update: 2021.10.18</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a776989",
   "metadata": {},
   "source": [
    "- 출처 : https://www.kaggle.com/somang1418/tuning-hyperparameters-under-10-minutes-lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb9c7512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d38ae41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg  width=\"550\" height=\"55\"><rect x=\"0\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#00798c;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"55\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#d1495b;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"110\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#edae49;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"165\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#66a182;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"220\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#4a4a4a;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"275\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#1a508b;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"330\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#e3120b;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"385\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#c5a880;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"440\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#9f5f80;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"495\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#6f9eaf;stroke-width:2;stroke:rgb(255,255,255)\"/></svg>"
      ],
      "text/plain": [
       "[(0.0, 0.4745098039215686, 0.5490196078431373),\n",
       " (0.8196078431372549, 0.28627450980392155, 0.3568627450980392),\n",
       " (0.9294117647058824, 0.6823529411764706, 0.28627450980392155),\n",
       " (0.4, 0.6313725490196078, 0.5098039215686274),\n",
       " (0.2901960784313726, 0.2901960784313726, 0.2901960784313726),\n",
       " (0.10196078431372549, 0.3137254901960784, 0.5450980392156862),\n",
       " (0.8901960784313725, 0.07058823529411765, 0.043137254901960784),\n",
       " (0.7725490196078432, 0.6588235294117647, 0.5019607843137255),\n",
       " (0.6235294117647059, 0.37254901960784315, 0.5019607843137255),\n",
       " (0.43529411764705883, 0.6196078431372549, 0.6862745098039216)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = [\"#00798c\", \"#d1495b\", '#edae49', '#66a182', '#4a4a4a',\n",
    "          '#1a508b', '#e3120b', '#c5a880', '#9F5F80', '#6F9EAF',\n",
    "          '#0278ae','#F39233', '#A7C5EB', '#54E346', '#ABCE74',\n",
    "        '#d6b0b1', '#58391c', '#cdd0cb', '#ffb396', '#6930c3']\n",
    "sns.color_palette(colors[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c840b855",
   "metadata": {},
   "source": [
    "산탄데르 데이터셋을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74fb3419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Downloading bayesian-optimization-1.2.0.tar.gz (14 kB)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from bayesian-optimization) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (2.1.0)\n",
      "Building wheels for collected packages: bayesian-optimization\n",
      "  Building wheel for bayesian-optimization (setup.py): started\n",
      "  Building wheel for bayesian-optimization (setup.py): finished with status 'done'\n",
      "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-py3-none-any.whl size=11685 sha256=ed44756252ecd8d869cc76c70f6f44c75fc40473506662a46c863e0dba97c404\n",
      "  Stored in directory: c:\\users\\km.yu99\\appdata\\local\\pip\\cache\\wheels\\37\\fa\\19\\f93e793d3944567a60b3ab93b446cf7370cc82c60c1d1c613f\n",
      "Successfully built bayesian-optimization\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d839a26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-optimize\n",
      "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from scikit-optimize) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.19.5)\n",
      "Collecting pyaml>=16.9\n",
      "  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.0.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (5.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->scikit-optimize) (2.1.0)\n",
      "Installing collected packages: pyaml, scikit-optimize\n",
      "Successfully installed pyaml-21.10.1 scikit-optimize-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca78667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning hyperparameters\n",
    "from bayes_opt import BayesianOptimization\n",
    "from skopt  import BayesSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a9562f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "276e1957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Downloading shap-0.39.0-cp38-cp38-win_amd64.whl (414 kB)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from shap) (1.6.0)\n",
      "Collecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from shap) (0.24.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from shap) (1.6.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from shap) (1.2.4)\n",
      "Requirement already satisfied: numba in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from shap) (0.53.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from shap) (1.19.5)\n",
      "Requirement already satisfied: tqdm>4.25.0 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from shap) (4.59.0)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from numba->shap) (0.36.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from numba->shap) (52.0.0.post20210125)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from pandas->shap) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from pandas->shap) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\km.yu99\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (1.0.1)\n",
      "Installing collected packages: slicer, shap\n",
      "Successfully installed shap-0.39.0 slicer-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b3fb614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics \n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fe3abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c44824",
   "metadata": {},
   "source": [
    "### 데이터 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f11d355d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 78.01 Mb (74.7% reduction)\n",
      "Mem. usage decreased to 77.82 Mb (74.6% reduction)\n",
      "Shape of train set:  (200000, 202)\n",
      "Shape of test set:  (200000, 201)\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train= reduce_mem_usage(pd.read_csv(\"data/santander/train.csv\"))\n",
    "test= reduce_mem_usage(pd.read_csv(\"data/santander/test.csv\"))\n",
    "print(\"Shape of train set: \",train.shape)\n",
    "print(\"Shape of test set: \",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bd1ebc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.921875</td>\n",
       "      <td>-6.785156</td>\n",
       "      <td>11.906250</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>11.460938</td>\n",
       "      <td>-9.281250</td>\n",
       "      <td>5.117188</td>\n",
       "      <td>18.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.433594</td>\n",
       "      <td>3.964844</td>\n",
       "      <td>3.136719</td>\n",
       "      <td>1.691406</td>\n",
       "      <td>18.515625</td>\n",
       "      <td>-2.398438</td>\n",
       "      <td>7.878906</td>\n",
       "      <td>8.562500</td>\n",
       "      <td>12.781250</td>\n",
       "      <td>-1.091797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>-4.148438</td>\n",
       "      <td>13.859375</td>\n",
       "      <td>5.390625</td>\n",
       "      <td>12.359375</td>\n",
       "      <td>7.042969</td>\n",
       "      <td>5.621094</td>\n",
       "      <td>16.531250</td>\n",
       "      <td>...</td>\n",
       "      <td>7.640625</td>\n",
       "      <td>7.722656</td>\n",
       "      <td>2.583984</td>\n",
       "      <td>10.953125</td>\n",
       "      <td>15.429688</td>\n",
       "      <td>2.033203</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>8.789062</td>\n",
       "      <td>18.359375</td>\n",
       "      <td>1.952148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.609375</td>\n",
       "      <td>-2.746094</td>\n",
       "      <td>12.078125</td>\n",
       "      <td>7.894531</td>\n",
       "      <td>10.585938</td>\n",
       "      <td>-9.085938</td>\n",
       "      <td>6.941406</td>\n",
       "      <td>14.617188</td>\n",
       "      <td>...</td>\n",
       "      <td>2.906250</td>\n",
       "      <td>9.789062</td>\n",
       "      <td>1.669922</td>\n",
       "      <td>1.685547</td>\n",
       "      <td>21.609375</td>\n",
       "      <td>3.142578</td>\n",
       "      <td>-6.519531</td>\n",
       "      <td>8.265625</td>\n",
       "      <td>14.718750</td>\n",
       "      <td>0.396484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.062500</td>\n",
       "      <td>-2.152344</td>\n",
       "      <td>8.953125</td>\n",
       "      <td>7.195312</td>\n",
       "      <td>12.585938</td>\n",
       "      <td>-1.835938</td>\n",
       "      <td>5.843750</td>\n",
       "      <td>14.921875</td>\n",
       "      <td>...</td>\n",
       "      <td>4.464844</td>\n",
       "      <td>4.742188</td>\n",
       "      <td>0.717773</td>\n",
       "      <td>1.421875</td>\n",
       "      <td>23.031250</td>\n",
       "      <td>-1.270508</td>\n",
       "      <td>-2.927734</td>\n",
       "      <td>10.289062</td>\n",
       "      <td>17.968750</td>\n",
       "      <td>-9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.835938</td>\n",
       "      <td>-1.483398</td>\n",
       "      <td>12.875000</td>\n",
       "      <td>6.636719</td>\n",
       "      <td>12.273438</td>\n",
       "      <td>2.449219</td>\n",
       "      <td>5.941406</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.490234</td>\n",
       "      <td>9.523438</td>\n",
       "      <td>-0.150757</td>\n",
       "      <td>9.195312</td>\n",
       "      <td>13.289062</td>\n",
       "      <td>-1.511719</td>\n",
       "      <td>3.925781</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>-8.812500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target      var_0     var_1      var_2     var_3      var_4  \\\n",
       "0  train_0       0   8.921875 -6.785156  11.906250  5.093750  11.460938   \n",
       "1  train_1       0  11.500000 -4.148438  13.859375  5.390625  12.359375   \n",
       "2  train_2       0   8.609375 -2.746094  12.078125  7.894531  10.585938   \n",
       "3  train_3       0  11.062500 -2.152344   8.953125  7.195312  12.585938   \n",
       "4  train_4       0   9.835938 -1.483398  12.875000  6.636719  12.273438   \n",
       "\n",
       "      var_5     var_6      var_7  ...   var_190   var_191   var_192  \\\n",
       "0 -9.281250  5.117188  18.625000  ...  4.433594  3.964844  3.136719   \n",
       "1  7.042969  5.621094  16.531250  ...  7.640625  7.722656  2.583984   \n",
       "2 -9.085938  6.941406  14.617188  ...  2.906250  9.789062  1.669922   \n",
       "3 -1.835938  5.843750  14.921875  ...  4.464844  4.742188  0.717773   \n",
       "4  2.449219  5.941406  19.250000  ... -1.490234  9.523438 -0.150757   \n",
       "\n",
       "     var_193    var_194   var_195   var_196    var_197    var_198   var_199  \n",
       "0   1.691406  18.515625 -2.398438  7.878906   8.562500  12.781250 -1.091797  \n",
       "1  10.953125  15.429688  2.033203  8.125000   8.789062  18.359375  1.952148  \n",
       "2   1.685547  21.609375  3.142578 -6.519531   8.265625  14.718750  0.396484  \n",
       "3   1.421875  23.031250 -1.270508 -2.927734  10.289062  17.968750 -9.000000  \n",
       "4   9.195312  13.289062 -1.511719  3.925781   9.500000  18.000000 -8.812500  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ac2b0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    179902\n",
       "1     20098\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7162b68",
   "metadata": {},
   "source": [
    "### Bayesian Optimization with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ccab1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=train['target']\n",
    "X=train.drop(['ID_code','target'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c672f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6,n_estimators=10000, output_process=False):\n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n",
    "    # parameters\n",
    "    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n",
    "        params = {'application':'binary', 'metric':'auc'}\n",
    "        params['learning_rate'] = max(min(learning_rate, 1), 0)\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['max_bin'] = int(round(max_depth))\n",
    "        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
    "        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "        \n",
    "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n",
    "        return max(cv_result['auc-mean'])\n",
    "     \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n",
    "                                            'num_leaves': (24, 80),\n",
    "                                            'feature_fraction': (0.1, 0.9),\n",
    "                                            'bagging_fraction': (0.8, 1),\n",
    "                                            'max_depth': (5, 30),\n",
    "                                            'max_bin':(20,90),\n",
    "                                            'min_data_in_leaf': (20, 80),\n",
    "                                            'min_sum_hessian_in_leaf':(0,100),\n",
    "                                           'subsample': (0.01, 1.0)}, random_state=200)\n",
    "\n",
    "    \n",
    "    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
    "    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n",
    "    \n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    model_auc=[]\n",
    "    for model in range(len( lgbBO.res)):\n",
    "        model_auc.append(lgbBO.res[model]['target'])\n",
    "    \n",
    "    # return best parameters\n",
    "    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61e23dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4791\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4791\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4791\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n",
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8441  \u001b[0m | \u001b[0m 0.9895  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 20.17   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2600\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2600\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2600\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n",
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.79    \u001b[0m | \u001b[0m 0.9964  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 70.77   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5388\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5388\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5388\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n",
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8276  \u001b[0m | \u001b[0m 0.8192  \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 54.7    \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4791\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038895 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4791\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4791\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n",
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.865   \u001b[0m | \u001b[95m 0.9281  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 60.78   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n",
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8238  \u001b[0m | \u001b[0m 0.9946  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 45.14   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8333129803667263, subsample=0.2623276224265793 will be ignored. Current value: bagging_fraction=0.8333129803667263\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8333129803667263, subsample=0.2623276224265793 will be ignored. Current value: bagging_fraction=0.8333129803667263\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8333129803667263, subsample=0.2623276224265793 will be ignored. Current value: bagging_fraction=0.8333129803667263\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8333129803667263, subsample=0.2623276224265793 will be ignored. Current value: bagging_fraction=0.8333129803667263\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4990\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8333129803667263, subsample=0.2623276224265793 will be ignored. Current value: bagging_fraction=0.8333129803667263\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8333129803667263, subsample=0.2623276224265793 will be ignored. Current value: bagging_fraction=0.8333129803667263\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8333129803667263, subsample=0.2623276224265793 will be ignored. Current value: bagging_fraction=0.8333129803667263\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4990\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8333129803667263, subsample=0.2623276224265793 will be ignored. Current value: bagging_fraction=0.8333129803667263\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8333129803667263, subsample=0.2623276224265793 will be ignored. Current value: bagging_fraction=0.8333129803667263\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8333129803667263, subsample=0.2623276224265793 will be ignored. Current value: bagging_fraction=0.8333129803667263\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4990\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8333129803667263, subsample=0.2623276224265793 will be ignored. Current value: bagging_fraction=0.8333129803667263\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n",
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8515  \u001b[0m | \u001b[0m 0.8333  \u001b[0m | \u001b[0m 0.7175  \u001b[0m | \u001b[0m 0.06083 \u001b[0m | \u001b[0m 87.46   \u001b[0m | \u001b[0m 25.17   \u001b[0m | \u001b[0m 62.81   \u001b[0m | \u001b[0m 32.9    \u001b[0m | \u001b[0m 29.53   \u001b[0m | \u001b[0m 0.2623  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8333129803667263, subsample=0.2623276224265793 will be ignored. Current value: bagging_fraction=0.8333129803667263\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044441 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3597\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3597\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3597\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n",
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8628  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.735   \u001b[0m | \u001b[0m 0.397   \u001b[0m | \u001b[0m 87.79   \u001b[0m | \u001b[0m 17.55   \u001b[0m | \u001b[0m 51.9    \u001b[0m | \u001b[0m 34.75   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5985\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5985\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5985\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n",
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8141  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 48.96   \u001b[0m | \u001b[0m 21.6    \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3199\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3199\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3199\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n",
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.8678  \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 0.1     \u001b[0m | \u001b[95m 0.349   \u001b[0m | \u001b[95m 87.44   \u001b[0m | \u001b[95m 15.73   \u001b[0m | \u001b[95m 62.14   \u001b[0m | \u001b[95m 40.4    \u001b[0m | \u001b[95m 24.0    \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5189\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041912 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5189\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5189\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m 0.87    \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 0.9     \u001b[0m | \u001b[95m 0.1719  \u001b[0m | \u001b[95m 81.38   \u001b[0m | \u001b[95m 26.17   \u001b[0m | \u001b[95m 55.8    \u001b[0m | \u001b[95m 46.83   \u001b[0m | \u001b[95m 24.0    \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3398\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3398\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3398\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n",
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8487  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.8141  \u001b[0m | \u001b[0m 0.6536  \u001b[0m | \u001b[0m 69.51   \u001b[0m | \u001b[0m 17.42   \u001b[0m | \u001b[0m 59.87   \u001b[0m | \u001b[0m 37.5    \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5985\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5985\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5985\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n",
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8604  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.5791  \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 67.14   \u001b[0m | \u001b[0m 57.19   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3398\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3398\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3398\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n",
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7655  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 17.11   \u001b[0m | \u001b[0m 47.18   \u001b[0m | \u001b[0m 59.99   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5985\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5985\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035716 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5985\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n",
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.8462  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 79.49   \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 68.9    \u001b[0m | \u001b[0m 45.58   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9183904394574285, subsample=0.530640637441884 will be ignored. Current value: bagging_fraction=0.9183904394574285\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9183904394574285, subsample=0.530640637441884 will be ignored. Current value: bagging_fraction=0.9183904394574285\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9183904394574285, subsample=0.530640637441884 will be ignored. Current value: bagging_fraction=0.9183904394574285\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9183904394574285, subsample=0.530640637441884 will be ignored. Current value: bagging_fraction=0.9183904394574285\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5388\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9183904394574285, subsample=0.530640637441884 will be ignored. Current value: bagging_fraction=0.9183904394574285\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9183904394574285, subsample=0.530640637441884 will be ignored. Current value: bagging_fraction=0.9183904394574285\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9183904394574285, subsample=0.530640637441884 will be ignored. Current value: bagging_fraction=0.9183904394574285\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 13398, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5388\n",
      "[LightGBM] [Info] Number of data points in the train set: 133333, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9183904394574285, subsample=0.530640637441884 will be ignored. Current value: bagging_fraction=0.9183904394574285\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9183904394574285, subsample=0.530640637441884 will be ignored. Current value: bagging_fraction=0.9183904394574285\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9183904394574285, subsample=0.530640637441884 will be ignored. Current value: bagging_fraction=0.9183904394574285\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 13399, number of negative: 119935\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5388\n",
      "[LightGBM] [Info] Number of data points in the train set: 133334, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9183904394574285, subsample=0.530640637441884 will be ignored. Current value: bagging_fraction=0.9183904394574285\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191762\n",
      "[LightGBM] [Info] Start training from score -2.191762\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100485 -> initscore=-2.191844\n",
      "[LightGBM] [Info] Start training from score -2.191844\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100492 -> initscore=-2.191770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score -2.191770\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.8442  \u001b[0m | \u001b[0m 0.9184  \u001b[0m | \u001b[0m 0.5808  \u001b[0m | \u001b[0m 0.7906  \u001b[0m | \u001b[0m 74.01   \u001b[0m | \u001b[0m 27.36   \u001b[0m | \u001b[0m 45.6    \u001b[0m | \u001b[0m 37.98   \u001b[0m | \u001b[0m 31.09   \u001b[0m | \u001b[0m 0.5306  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "opt_params = bayes_parameter_opt_lgb(X, y, init_round=5, opt_round=10, n_folds=3, random_seed=6,n_estimators=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "287b41d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.869950989798792,\n",
       " {'bagging_fraction': 1.0,\n",
       "  'feature_fraction': 0.9,\n",
       "  'learning_rate': 0.1718549530829431,\n",
       "  'max_bin': 81.37983603860036,\n",
       "  'max_depth': 26.16709557494532,\n",
       "  'min_data_in_leaf': 55.797187511967564,\n",
       "  'min_sum_hessian_in_leaf': 46.82805655404337,\n",
       "  'num_leaves': 24.0,\n",
       "  'subsample': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39a638c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 1.0,\n",
       " 'feature_fraction': 0.9,\n",
       " 'learning_rate': 0.1718549530829431,\n",
       " 'max_bin': 81.37983603860036,\n",
       " 'max_depth': 26.16709557494532,\n",
       " 'min_data_in_leaf': 55.797187511967564,\n",
       " 'min_sum_hessian_in_leaf': 46.82805655404337,\n",
       " 'num_leaves': 24.0,\n",
       " 'subsample': 1.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a8137",
   "metadata": {},
   "source": [
    "파라미터를 추가하여 최적의 파라미터를 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97c7d08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 1.0,\n",
       " 'feature_fraction': 0.9,\n",
       " 'learning_rate': 0.1718549530829431,\n",
       " 'max_bin': 81,\n",
       " 'max_depth': 26,\n",
       " 'min_data_in_leaf': 56,\n",
       " 'min_sum_hessian_in_leaf': 46.82805655404337,\n",
       " 'num_leaves': 24,\n",
       " 'subsample': 1.0,\n",
       " 'objective': 'binary',\n",
       " 'metric': 'auc',\n",
       " 'is_unbalance': True,\n",
       " 'boost_from_average': False}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
    "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
    "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
    "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
    "opt_params[1]['objective']='binary'\n",
    "opt_params[1]['metric']='auc'\n",
    "opt_params[1]['is_unbalance']=True\n",
    "opt_params[1]['boost_from_average']=False\n",
    "opt_params=opt_params[1]\n",
    "opt_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b34475b",
   "metadata": {},
   "source": [
    "### Training LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4679b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=train['target']\n",
    "features= [c for c in train.columns if c not in ['target','ID_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4f7b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=31416)\n",
    "oof = np.zeros(len(train))\n",
    "predictions = np.zeros(len(test))\n",
    "feature_importance_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f086295a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 18089, number of negative: 161911\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16134\n",
      "[LightGBM] [Info] Number of data points in the train set: 180000, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[500]\ttraining's auc: 0.988497\tvalid_1's auc: 0.883424\n",
      "Early stopping, best iteration is:\n",
      "[268]\ttraining's auc: 0.967482\tvalid_1's auc: 0.88736\n",
      "Fold 1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 18089, number of negative: 161911\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16134\n",
      "[LightGBM] [Info] Number of data points in the train set: 180000, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[500]\ttraining's auc: 0.988235\tvalid_1's auc: 0.884859\n",
      "Early stopping, best iteration is:\n",
      "[279]\ttraining's auc: 0.968394\tvalid_1's auc: 0.887545\n",
      "Fold 2\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 18088, number of negative: 161912\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16134\n",
      "[LightGBM] [Info] Number of data points in the train set: 180000, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[228]\ttraining's auc: 0.961216\tvalid_1's auc: 0.883048\n",
      "Fold 3\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 18088, number of negative: 161912\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16134\n",
      "[LightGBM] [Info] Number of data points in the train set: 180000, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[231]\ttraining's auc: 0.961759\tvalid_1's auc: 0.890229\n",
      "Fold 4\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 18088, number of negative: 161912\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16134\n",
      "[LightGBM] [Info] Number of data points in the train set: 180000, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[232]\ttraining's auc: 0.962126\tvalid_1's auc: 0.885536\n",
      "Fold 5\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 18088, number of negative: 161912\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058712 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16134\n",
      "[LightGBM] [Info] Number of data points in the train set: 180000, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[233]\ttraining's auc: 0.962216\tvalid_1's auc: 0.884955\n",
      "Fold 6\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 18088, number of negative: 161912\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058288 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16134\n",
      "[LightGBM] [Info] Number of data points in the train set: 180000, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[239]\ttraining's auc: 0.962592\tvalid_1's auc: 0.889757\n",
      "Fold 7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 18088, number of negative: 161912\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057592 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16134\n",
      "[LightGBM] [Info] Number of data points in the train set: 180000, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[500]\ttraining's auc: 0.988383\tvalid_1's auc: 0.885847\n",
      "Early stopping, best iteration is:\n",
      "[264]\ttraining's auc: 0.966631\tvalid_1's auc: 0.888346\n",
      "Fold 8\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 18088, number of negative: 161912\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16134\n",
      "[LightGBM] [Info] Number of data points in the train set: 180000, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[500]\ttraining's auc: 0.988453\tvalid_1's auc: 0.879937\n",
      "Early stopping, best iteration is:\n",
      "[263]\ttraining's auc: 0.96687\tvalid_1's auc: 0.881967\n",
      "Fold 9\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 18088, number of negative: 161912\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16134\n",
      "[LightGBM] [Info] Number of data points in the train set: 180000, number of used features: 200\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[500]\ttraining's auc: 0.988362\tvalid_1's auc: 0.885062\n",
      "Early stopping, best iteration is:\n",
      "[358]\ttraining's auc: 0.977853\tvalid_1's auc: 0.886413\n"
     ]
    }
   ],
   "source": [
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "\n",
    "    num_round = 15000\n",
    "    clf = lgb.train(opt_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, \n",
    "                    early_stopping_rounds = 250)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db13c687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.88637 \n"
     ]
    }
   ],
   "source": [
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9794585d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>var_0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>var_1</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>var_2</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>var_3</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>var_4</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>var_195</td>\n",
       "      <td>48</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>var_196</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>var_197</td>\n",
       "      <td>56</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>var_198</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>var_199</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  importance  fold\n",
       "0      var_0          39     1\n",
       "1      var_1          58     1\n",
       "2      var_2          48     1\n",
       "3      var_3          15     1\n",
       "4      var_4          17     1\n",
       "..       ...         ...   ...\n",
       "195  var_195          48    10\n",
       "196  var_196          41    10\n",
       "197  var_197          56    10\n",
       "198  var_198          65    10\n",
       "199  var_199          40    10\n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dc55dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>var_0</th>\n",
       "      <td>42.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_1</th>\n",
       "      <td>52.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_10</th>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_100</th>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_101</th>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_95</th>\n",
       "      <td>38.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_96</th>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_97</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_98</th>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_99</th>\n",
       "      <td>51.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         importance\n",
       "Feature            \n",
       "var_0          42.3\n",
       "var_1          52.1\n",
       "var_10          4.7\n",
       "var_100         7.4\n",
       "var_101        12.5\n",
       "...             ...\n",
       "var_95         38.9\n",
       "var_96          5.8\n",
       "var_97         24.0\n",
       "var_98          5.6\n",
       "var_99         51.1\n",
       "\n",
       "[200 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_df[[\"Feature\", \"importance\"]].groupby(\"Feature\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d9070ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:20].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a0191d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var_174', 'var_6', 'var_34', 'var_33', 'var_53', 'var_166', 'var_21',\n",
       "       'var_22', 'var_78', 'var_76', 'var_110', 'var_165', 'var_1', 'var_190',\n",
       "       'var_13', 'var_99', 'var_26', 'var_12', 'var_146', 'var_170'],\n",
       "      dtype='object', name='Feature')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1813fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2253323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,15))\n",
    "sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('Features importance (averaged/folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Feature_Importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12a32ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x684 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "explainer = shap.TreeExplainer(clf)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9f1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
