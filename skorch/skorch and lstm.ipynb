{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with Skorch and RNN with LSTM cells"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://discuss.pytorch.org/t/problem-with-skorch-and-rnn-with-lstm-cells/80674/1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <b>Author : Kwang Myung Yu</b></div>\n",
    "<div style=\"text-align: right\"> Initial upload: 2023. 7. 10</div>\n",
    "<div style=\"text-align: right\"> Last update: 2023. 7.10</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "#plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from skorch import NeuralNetRegressor\n",
    "import unittest\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_regr, y_regr = make_regression(1000, 20, n_informative=10, random_state=0)\n",
    "X_regr = X_regr.astype(np.float32)\n",
    "y_regr = y_regr.astype(np.float32) / 100\n",
    "y_regr = y_regr.reshape(-1, 1)\n",
    "ni = 20\n",
    "no = 1\n",
    "nh = 10\n",
    "nlayers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ni = 6,\n",
    "        no = 3,\n",
    "        nh = 10,\n",
    "        nlayers = 1\n",
    "    ):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        \n",
    "        self.ni = ni\n",
    "        self.no = no\n",
    "        self.nh = nh\n",
    "        self.nlayers = nlayers\n",
    "        \n",
    "        self.lstms = nn.ModuleList(\n",
    "            [nn.LSTMCell(self.ni, self.nh)] + [nn.LSTMCell(self.nh, self.nh) for i in range(nlayers - 1)]\n",
    "        )\n",
    "        self.out = nn.Linear(self.nh, self.no)\n",
    "        self.actfn = nn.Tanh()\n",
    "        # self.device = torch.device('cpu')\n",
    "        self.dtype = torch.float\n",
    "        \n",
    "    def forward(self, x, h0 = None, train = False):\n",
    "        hs = x # initialize hidden state\n",
    "        if h0 in None:\n",
    "            # h = torch.zeros(hs.shape[0], self.nh, device=device)\n",
    "            # c = torch.zeros(hs.shape[0], self.nh, device=device)\n",
    "            h = torch.zeros(hs.shape[0], self.nh)\n",
    "            c = torch.zeros(hs.shape[0], self.nh)\n",
    "        else:\n",
    "            (h, c) = h0\n",
    "            \n",
    "        # LSTM cells\n",
    "        for i in range(self.nlayers):\n",
    "            h, c = self.lstms[i](hs, (h, c))\n",
    "            if train:\n",
    "                hs = self.do(h)\n",
    "            else:\n",
    "                hs = h\n",
    "        y = self.out(hs)\n",
    "        return y, (h, c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 : LSTM, LSTMCell의 차이\n",
    "- https://discuss.pytorch.kr/t/nn-rnn-nn-rnncell/214/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_regr, y_regr = make_regression(1000, 20, n_informative=10, random_state=0)\n",
    "X_regr = X_regr.astype(np.float32)\n",
    "y_regr = y_regr.astype(np.float32) / 100\n",
    "y_regr = y_regr.reshape(-1, 1)\n",
    "ni = 20\n",
    "no = 1\n",
    "nh = 10\n",
    "nlayers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_regr = NeuralNetRegressor(\n",
    "    module=MyLSTM,\n",
    "    module__ni=ni,\n",
    "    module__no=no,\n",
    "    module__nh=nh,\n",
    "    module__nlayers=nlayers,\n",
    "    max_epochs=20,\n",
    "    lr=0.1,\n",
    "    #     device='cuda',  # uncomment this to train with CUDA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m net_regr\u001b[39m.\u001b[39;49mfit(X_regr,y_regr)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/skorch/regressor.py:82\u001b[0m, in \u001b[0;36mNeuralNetRegressor.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"See ``NeuralNet.fit``.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[39mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m# pylint: disable=useless-super-delegation\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m# this is actually a pylint bug:\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m# https://github.com/PyCQA/pylint/issues/1085\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(NeuralNetRegressor, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/skorch/net.py:1302\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarm_start \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialized_:\n\u001b[1;32m   1300\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialize()\n\u001b[0;32m-> 1302\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1303\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/skorch/net.py:1261\u001b[0m, in \u001b[0;36mNeuralNet.partial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m'\u001b[39m\u001b[39mon_train_begin\u001b[39m\u001b[39m'\u001b[39m, X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my)\n\u001b[1;32m   1260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1262\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1263\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/skorch/net.py:1173\u001b[0m, in \u001b[0;36mNeuralNet.fit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m   1171\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m'\u001b[39m\u001b[39mon_epoch_begin\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mon_epoch_kwargs)\n\u001b[0;32m-> 1173\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single_epoch(iterator_train, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, prefix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1174\u001b[0m                           step_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1176\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_single_epoch(iterator_valid, training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1177\u001b[0m                           step_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_step, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m   1179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m\"\u001b[39m\u001b[39mon_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mon_epoch_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/skorch/net.py:1209\u001b[0m, in \u001b[0;36mNeuralNet.run_single_epoch\u001b[0;34m(self, iterator, training, prefix, step_fn, **fit_params)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m iterator:\n\u001b[1;32m   1208\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m\"\u001b[39m\u001b[39mon_batch_begin\u001b[39m\u001b[39m\"\u001b[39m, batch\u001b[39m=\u001b[39mbatch, training\u001b[39m=\u001b[39mtraining)\n\u001b[0;32m-> 1209\u001b[0m     step \u001b[39m=\u001b[39m step_fn(batch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1210\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mrecord_batch(prefix \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_loss\u001b[39m\u001b[39m\"\u001b[39m, step[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mitem())\n\u001b[1;32m   1211\u001b[0m     batch_size \u001b[39m=\u001b[39m (get_len(batch[\u001b[39m0\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(batch, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m))\n\u001b[1;32m   1212\u001b[0m                   \u001b[39melse\u001b[39;00m get_len(batch))\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/skorch/net.py:1088\u001b[0m, in \u001b[0;36mNeuralNet.train_step\u001b[0;34m(self, batch, **fit_params)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\n\u001b[1;32m   1081\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mon_grad_computed\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1082\u001b[0m         named_parameters\u001b[39m=\u001b[39mTeeGenerator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_all_learnable_params()),\n\u001b[1;32m   1083\u001b[0m         batch\u001b[39m=\u001b[39mbatch,\n\u001b[1;32m   1084\u001b[0m         training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1085\u001b[0m     )\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m step[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_optimizer(step_fn)\n\u001b[1;32m   1089\u001b[0m \u001b[39mreturn\u001b[39;00m step_accumulator\u001b[39m.\u001b[39mget_step()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/skorch/net.py:1043\u001b[0m, in \u001b[0;36mNeuralNet._step_optimizer\u001b[0;34m(self, step_fn)\u001b[0m\n\u001b[1;32m   1041\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m   1042\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(step_fn)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/optim/sgd.py:67\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m---> 67\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m     69\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m     70\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/skorch/net.py:1077\u001b[0m, in \u001b[0;36mNeuralNet.train_step.<locals>.step_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_fn\u001b[39m():\n\u001b[1;32m   1076\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad_optimizer()\n\u001b[0;32m-> 1077\u001b[0m     step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step_single(batch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1078\u001b[0m     step_accumulator\u001b[39m.\u001b[39mstore_step(step)\n\u001b[1;32m   1080\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\n\u001b[1;32m   1081\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mon_grad_computed\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1082\u001b[0m         named_parameters\u001b[39m=\u001b[39mTeeGenerator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_all_learnable_params()),\n\u001b[1;32m   1083\u001b[0m         batch\u001b[39m=\u001b[39mbatch,\n\u001b[1;32m   1084\u001b[0m         training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1085\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/skorch/net.py:976\u001b[0m, in \u001b[0;36mNeuralNet.train_step_single\u001b[0;34m(self, batch, **fit_params)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_training(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    975\u001b[0m Xi, yi \u001b[39m=\u001b[39m unpack_data(batch)\n\u001b[0;32m--> 976\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(Xi, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    977\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_loss(y_pred, yi, X\u001b[39m=\u001b[39mXi, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    978\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/skorch/net.py:1504\u001b[0m, in \u001b[0;36mNeuralNet.infer\u001b[0;34m(self, x, **fit_params)\u001b[0m\n\u001b[1;32m   1502\u001b[0m     x_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_x_and_fit_params(x, fit_params)\n\u001b[1;32m   1503\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mx_dict)\n\u001b[0;32m-> 1504\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule_(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m, in \u001b[0;36mMyLSTM.forward\u001b[0;34m(self, x, h0, train)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, h0 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, train \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     25\u001b[0m     hs \u001b[39m=\u001b[39m x \u001b[39m# initialize hidden state\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mif\u001b[39;00m h0 \u001b[39min\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n\u001b[1;32m     27\u001b[0m         \u001b[39m# h = torch.zeros(hs.shape[0], self.nh, device=device)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         \u001b[39m# c = torch.zeros(hs.shape[0], self.nh, device=device)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(hs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnh)\n\u001b[1;32m     30\u001b[0m         c \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(hs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnh)\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "net_regr.fit(X_regr,y_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, ni=6, no=3, nh=10, nlayers=1):\n",
    "        super(MyLSTM, self).__init__()\n",
    "\n",
    "        self.ni = ni\n",
    "        self.no = no\n",
    "        self.nh = nh\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "        self.lstms = nn.ModuleList(\n",
    "            [nn.LSTMCell(self.ni, self.nh)] + [nn.LSTMCell(self.nh, self.nh) for i in range(nlayers - 1)])\n",
    "        self.out = nn.Linear(self.nh, self.no)\n",
    "        self.do = nn.Dropout(p=0.2)\n",
    "        self.actfn = nn.Tanh()\n",
    "        #self.device = torch.device('cpu')\n",
    "        self.dtype = torch.float\n",
    "\n",
    "    # description of the whole block\n",
    "    def forward(self, x, h0=None, train=False):\n",
    "        hs = x  # initiate hidden state\n",
    "        if h0 is None:\n",
    "            h = torch.zeros(hs.shape[0], self.nh)\n",
    "            c = torch.zeros(hs.shape[0], self.nh)\n",
    "        else:\n",
    "            (h, c) = h0\n",
    "\n",
    "        # LSTM cells\n",
    "        for i in range(self.nlayers):\n",
    "            h, c = self.lstms[i](hs, (h, c))\n",
    "            if train:\n",
    "                hs = self.do(h)\n",
    "            else:\n",
    "                hs = h\n",
    "        y = self.out(hs)\n",
    "        return y, (h, c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, skorch just passes the module's output to get_loss and, subsequently, to the criterion. You have several options:\n",
    "\n",
    "- create your own simple criterion that extracts the prediction and discards the context / hidden state\n",
    "- implement your own get_loss that extracts the prediction and discards the context / hidden state\n",
    "- don't return anything except the prediction\n",
    "- make a case for why we should only pass the first argument to get_loss / the criterion :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextlessMSE(torch.nn.MSELoss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y, (h, c) = y_pred # extract prediction and context information \n",
    "        return super().forward(y, y_true)\n",
    "\n",
    "net_regr = NeuralNetRegressor(\n",
    "    module=MyLSTM,\n",
    "    module__ni=ni,\n",
    "    module__no=no,\n",
    "    module__nh=nh,\n",
    "    module__nlayers=nlayers,\n",
    "    max_epochs=20,\n",
    "    lr=0.1,\n",
    "    criterion=ContextlessMSE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.6593\u001b[0m        \u001b[32m4.0033\u001b[0m  0.0128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m4.6153\u001b[0m        \u001b[32m3.9702\u001b[0m  0.0113\n",
      "      3        \u001b[36m4.5801\u001b[0m        \u001b[32m3.9338\u001b[0m  0.0133\n",
      "      4        \u001b[36m4.5287\u001b[0m        \u001b[32m3.8749\u001b[0m  0.0121\n",
      "      5        \u001b[36m4.4394\u001b[0m        \u001b[32m3.7641\u001b[0m  0.0113\n",
      "      6        \u001b[36m4.2622\u001b[0m        \u001b[32m3.5232\u001b[0m  0.0104\n",
      "      7        \u001b[36m3.8518\u001b[0m        \u001b[32m2.9143\u001b[0m  0.0111\n",
      "      8        \u001b[36m2.8290\u001b[0m        \u001b[32m1.5763\u001b[0m  0.0106\n",
      "      9        \u001b[36m1.2824\u001b[0m        \u001b[32m0.6832\u001b[0m  0.0102\n",
      "     10        \u001b[36m0.6700\u001b[0m        \u001b[32m0.5607\u001b[0m  0.0122\n",
      "     11        \u001b[36m0.5737\u001b[0m        \u001b[32m0.5160\u001b[0m  0.0102\n",
      "     12        \u001b[36m0.5184\u001b[0m        \u001b[32m0.4735\u001b[0m  0.0111\n",
      "     13        \u001b[36m0.4715\u001b[0m        \u001b[32m0.4303\u001b[0m  0.0104\n",
      "     14        \u001b[36m0.4213\u001b[0m        \u001b[32m0.3977\u001b[0m  0.0108\n",
      "     15        \u001b[36m0.3814\u001b[0m        \u001b[32m0.3695\u001b[0m  0.0102\n",
      "     16        0.3846        1.2951  0.0098\n",
      "     17        1.4625        0.3879  0.0112\n",
      "     18        \u001b[36m0.3738\u001b[0m        0.3934  0.0105\n",
      "     19        \u001b[36m0.3376\u001b[0m        \u001b[32m0.3379\u001b[0m  0.0111\n",
      "     20        \u001b[36m0.2983\u001b[0m        \u001b[32m0.3359\u001b[0m  0.0112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
       "  module_=MyLSTM(\n",
       "    (lstms): ModuleList(\n",
       "      (0): LSTMCell(20, 10)\n",
       "      (1-2): 2 x LSTMCell(10, 10)\n",
       "    )\n",
       "    (out): Linear(in_features=10, out_features=1, bias=True)\n",
       "    (do): Dropout(p=0.2, inplace=False)\n",
       "    (actfn): Tanh()\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_regr.fit(X_regr,y_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
